# siamo interessati a trovare qual è il task migliore -- not all handwriting tasks are equally important for disease detection, and a targeted selection can lead to more efficient and effective diagnostic tools.
# leggere libri su feature selection?
# sparse group lasso individua i task piu importanti e di ogni task, qual è la featura migliore
# From Richard Hastie's glmnet package vignette: "It is known that the ridge penalty shrinks the coefficients of correlated predictors towards each other while the lasso tends to pick one of them and discard the others."
# First, if two explanatory variables are highly correlated and effect sizes are unconstrained, then the explanatory variables show very similar influence on the response variable. In such a situation, the Lasso will only select one variable at random (Xu et al., 2012). This is problematic when the results are used for hypothesis generation because we would like to simultaneously select all variables which have the same evidence of activity. Here, we refer to this property as instability.
#sembra di aver letto che per elastic net e forse altri, X devono essere sulla stessa scala


You mentioned that your goal is understanding , not prediction. This means you're primarily interested in identifying which variables are most important for explaining the outcome, rather than optimizing the model's predictive performance.

forse loha spiegato a voce come ha fatto elastic net.. ascolta la lezione e se non ce chiedigli

si puo scegliere alpha per massimizzare recall e precision anche 

The cross-validated likelihood landscape has a very flat maximum ridge on the (α, λ) landscape. Hence, all these values would suffice. There’s a reason why the popular glmnet package2 does not provide a function to cross-validate α and λ jointly!


Biomarker Uncertainty Principle:
A molecular signature can be either parsimonious or predictive, but not both.
— FE Harrell, 2009



α is often chosen based on intent and domain knowledge. Do you want a sparse model with many zero coefficients? Then chose α
 close to a pure lasso regression. Do you want a dense model with many non-zero but small coefficients? Then chose α
 close to a pure ridge model.
 

 
 to quote Hastie and Zou, "If there is a group of variables among which the pairwise correlations are very high, then the lasso tends to select only one variable from the group and does not care which one is selected." This is a problem, for instance, because it means that we're not likely to find an element of the true support using the lasso--just one highly correlated with it
 


scaling prima di fare clustering?

controlla se tutti i metodi vogliono o non vogliono scaling


lambda param della funzione glmnet:
A user supplied lambda sequence. Typical usage is to have the program compute its own lambda sequence based on nlambda and lambda.min.ratio. Supplying a value of lambda overrides this. WARNING: use with care. Avoid supplying a single value for lambda (for predictions after CV use predict() instead). Supply instead a decreasing sequence of lambda values. glmnet relies on its warms starts for speed, and its often faster to fit a whole path than compute a single fit.


chatgpt consiglia
selected_features <- which(abs(coefficients) > 1e-6)



It is important to recognize the difficulty of the selection and estimation tasks by bootstrapping the whole process to put confidence intervals on such variable importance estimates. It amazes me how so many applied fields will insist on p-values or confidence intervals, only to forget about uncertainty when machine learning calculates feature importance. If you wouldn't accept a mean without some kind of test or interval estimate, why is feature importance exempt from this?--
nel senso che i coefficienti dipendono dal sample

i difetti sono che il modello è lineare e non considero le interazioni

The variable that enters the model first (i.e., becomes non-zero) as lambda decreases is typically the one with the strongest correlation with the response variable

# stable group lasso per **task**
> stable_groups <- which(max_selection_probabilities >= 0.8)
> stable_groups
 [1]  2  7  9 15 17 19 21 23 24 25
> stable_groups <- which(max_selection_probabilities >= 0.9)
> stable_groups
[1]  7  9 15 17 19 23
> stable_groups <- which(max_selection_probabilities >= 0.95)
> stable_groups
[1]  7  9 19 23

# group lasso stable **features**
[1]  1  2  5 11 16
(air_time, disp_dindex, max_x_extension, mean_jerk_on_paper, pressure mean)


# stable group lasso on **components** (nlam = 5)
> stable_groups <- which(max_selection_probabilities >= 0.9)
> stable_groups
 [1]  16  54  84  92 151 189 196 199 213 257
> stable_groups <- which(max_selection_probabilities >= 0.95)
> stable_groups
[1] 151 196 199 213

Stable Group 151 :
air_time15 total_time15 
Stable Group 196 :
num_of_pendown19 
Stable Group 199 :
pressure_var19 
Stable Group 213 :
disp_index21 






Conditional independence helps distinguish between direct and indirect relationships. Two variables might appear correlated (dependent) because they're both influenced by a third variable, not because they directly affect each other.
For example: Blood pressure and heart rate might appear related in your data, but perhaps they're both actually responding to exercise level. The graphical model would show connections from exercise to both variables, but no direct connection between blood pressure and heart rate.
se fai correlation graph tutte le tre variabili sono collegate (clique)
se fai graph lasso avrai un collegamento da physical activity a blood pressure
e un collegamento anche a heart rate
This distinction is clinically meaningful. A doctor seeing only the correlation might assume that treating high blood pressure would directly affect heart rate. The conditional independence graph reveals that both are responding to a common cause (physical activity), and interventions might be more effective if targeted at the activity level rather than assuming a direct relationship between BP and HR.

Standard correlations can be misleading because they capture both direct and indirect effects. The graph from GLasso focuses on the "true" structure by showing only direct effects.

No connections between {A,B,E} and {C,D}: The tech/consumer sector and the oil/airline sector operate independently.

Control vs. AD Differences: By running separate models for the control group and AD patients, you could identify which connections strengthen or weaken with disease progression.

Have you made a network using **correlations before**? Have you ever noticed that these networks are kind of clumpy?
What do I mean by clumpy? Consider three species of bacteria: A, B, and C. Suppose A secretes some molecule essential for the growth of B. Suppose also that C is a predator of A (maybe a bdellovibrio). What is the relationship between B and C? These microbes have no direct interaction in this scenario. Nevertheless, their abundances will be correlated. 
What does this mean for your network? It’s going to be chock full o’ triangles.
Is this a problem? You tell me - are you interested in identifying “true” direct interactions? Do you want indirect interactions to be shown by edges in your network? In general, if you are interested in analyzing network structure these indirect interactions may be a problem
So correlation might not work for us huh? It’s not correlation’s fault though. We weren’t asking the right question.
We asked: Are species A and B correlated?
What we should ask: Are species A and B correlated, given all the other species in the community?
What we are really interested in is the partial correlation - the correlation between A and B controlling for C.
For various technical reasons we don’t want to actually directly compute partial correlations between each pair of variables (see slide 60-onward in this lecture if you are curious). Instead we use a method called the “graphical lasso”.

colorare i nodi che appartengono allo stesso task?


if theta_ij != 0 suggests a direct relationship between Xi and Xj even after accounting for the influence of other variables (llm)
Ma non è come in linear regression dove beta_j rappresenta quanto y aumenta per ogni unità di X_j tenendo gli altri fissi??!!

Variables connected to a target variable in the graph may be important predictors in a regression model.(llm)



Suppose you computed a graph for a health dataset with variables like blood pressure, cholesterol, age, and BMI:
If there’s an edge between blood pressure and cholesterol, it suggests these two variables are conditionally dependent, even after accounting for age and BMI.
If BMI is a hub, it might indicate that BMI strongly influences other health metrics.(llm)

Conditional Dependence : An edge between variables X and Y indicates they are conditionally dependent given all other variables. This means their relationship is not fully explained by other variables in the model.(llm)


2. **Feature selection or dimensionality reduction**: Graphical Lasso could potentially help in identifying a subset of features that are most relevant for distinguishing between Alzheimer's patients and healthy controls by examining the sparsity pattern in the estimated precision matrix.

The resulting graph could identify **hub features** (highly connected nodes) that are critical markers of AD. For example, tremor metrics (GMRT) or speed variability might emerge as central to distinguishing AD patients from controls.


Central Hub: The variable may act as a key mediator or hub in the network, influencing or being influenced by many other variables. llm

AD affects both cognitive (memory, planning) and motor (fine movement control) functions

It strongly interacts with many other handwriting features, meaning it might be a fundamental descriptor of handwriting patterns.

If Total Time (TT) is a high-degree node in both AD and healthy groups, it means TT is just generally important for handwriting.

But if TT has more connections in the AD group, it suggests that handwriting slowing down is more influential in Alzheimer’s patients.

If a feature like Mean Jerk (MJP) is highly connected only in AD patients, it might indicate a specific signature of cognitive decline.

Even in a mixed dataset, a high-degree feature is statistically central to the network. This implies it is:
A hub that integrates multiple handwriting dimensions (e.g., time, pressure, spatial dispersion).
Potentially more sensitive to group differences because its connections aggregate subtle changes.

The node with the highest degree is a hub in the conditional dependency graph. It has the strongest or most numerous conditional relationships with other variables, meaning it is statistically dependent on many other variables even after accounting for the effects of all others.

 if two variables are correlated, this does not necessarily imply that they are directly dependent on each other
 
 Now it is not a priori clear if this dependency is due to a direct relationship between the two variables or if it is mediated by a **set** of other random variables Z
 
 In contrast to correlation, a vanishing partial correlation is not a strong indicator of independence, but high partial correlation is a strong indicator of dependence. (che vuoldire)
 
 However, an observed association between two variables might be misleading, i.e. not refecting a
genuine interaction between them

More precisely, in a partial correlation network, the edges connected to a single node are analogous to the
regression coefcients obtained in a multiple regression model, where the dependent variable is the node under
consideration30 Moreover, representing partial correlations, the edges are considered indicative of causal relations between the connecting nodes

Degree Centrality: Features with many connections may be broadly influential.

Estimate separate precision matrices (and their corresponding graphs) for Alzheimer’s patients and healthy controls. Identify edges (conditional dependencies) that differ significantly between the two groups. This highlights disrupted interactions between handwriting features in AD, such as loss of coordination between specific tasks or motor control mechanisms. llm

Centrality Measures : Calculate centrality measures such as degree, betweenness, closeness, and eigenvector centrality to identify key features that play a significant role in distinguishing Alzheimer's patients from healthy individuals. llm

Interpret communities in the context of the DARWIN tasks (e.g., motor control features vs. cognitive load features). llm

fare fit ols sulle feature trovate per vedere i segni dei beta!??

glmnet.fit Fit a GLM with elastic net regularization for a single value of lambda---credo che tu usi glmnet()



        1         2         3         4         5 
0.3218391 0.2758621 0.3563218 0.3390805 0.3620690 task e nfold=6 (min=2)

        1         2         3         4         5 
0.1666667 0.2701149 0.2988506 0.3505747 0.4080460 feature e nfold = 6 (min=1)

        1         2         3         4         5 
0.3401961 0.3496732 0.3264706 0.3617647 0.3398693 task nfold=10 (min = 3)

        1         2         3         4         5 
0.1836601 0.2888889 0.3388889 0.2898693 0.3790850 nfold = 10 (min = 1)


  # correlation_df <- data.frame(
  #   Var1 =     c("X1",  "X1",  "X1",  "X2",  "X2",  "X3"),
  #   Var2 =     c("X2",  "X3",  "X4",  "X3" , "X4",  "X4"),
  #   abs_value =c( 0,     0,      0,    1,      1,     0 )
  # )
  # S_alpha = c("X1","X3","X4")




One of the major drawbacks of Lasso is that it tends to se- lect only a small subset from a large group of strongly correlated co-variates -- (Identifying groups of strongly correlated variables through Smoothed Ordered Weighted l1-norms)

Using the l1-norm to regularize the estimation of the parameter vector of a linear model leads to an unstable estimator when covariates are highly correlated -- (Trace Lasso: a trace norm regularization for correlated designs)
 
In recent years, a large body of work has shown that the Lasso was performing optimally in high- dimensional low-correlation settings, both in terms of prediction [6], estimation of parameters or estimation of supports [7, 8]. However, most data exhibit strong correlations, with various correla- tion structures, such as clusters (i.e., close to block-diagonal covariance matrices) or sparse graphs, such as for example problems involving sequences (in which case, the covariance matrix is close to a Toeplitz matrix [9]). In these situations, the Lasso is known to have stability problems: although its predictive performance is not disastrous, the selected predictor may vary a lot (typically, given two correlated variables, the Lasso will only select one of the two, at random). -- (Trace Lasso: a trace norm regularization for correlated designs)


When the model includes several highly correlated variables, all of which are related to some extent to the response variable, lasso tends to pick only one or a few of them and shrinks the rest to 0. This may not be a desirable feature. For example, in microarray analysis, expression levels of genes that share one common biological pathway are usually highly correlated, and these genes may all contribute to the biological process, but lasso usually selects only one gene from the group. An ideal method should be able to select all relevant genes, highly correlated or not, while eliminating trivial genes.--(RANDOM LASSO)

However, Lasso performs erratically when features are correlated. This limits the use of such algorithms in biological problems, where features such as genes often work together in path- ways, leading to sets of highly correlated feature ----Therefore, Lasso will not discover all informative features when there is a strong correlation structure in the data.-- (2002.12460)

It is well-known that the ‘0-norm regularizer is optimal in variable selection, -- (bioinformatics_35_7_1181)

Gene expression profiles can result in unstable variable selection. As expression levels of genes within a regulatory pathway are highly correlated (Michalopoulos et al., 2012), the Lasso will be unstable for selection of variables when several of the variables participate in the same regulatory pathway. This can lead researchers to believe that a single element of the pathway is likely to be causal for a phenotype (based on variable selection), when in reality the evidence is shared between all elements of the pathway.
-- (bioinformatics_35_7_1181)

Selecting the true direct predictors of an outcome, relative to a large set of available variables, is a distinctly different problem from prediction accuracy and of much substantial interest in its own right
-- (Biometrical J - 2023 - Hanke - Variable selection in linear regression models  Choosing the best subset is not always the)

Collinearity is especially problematic when a model’s purpose is explanation rather than prediction --
As a disclaimer, variables do not need to be highly correlated for
multicollinearity to exist, though this is oftentimes the case
131_Final_Paper_PDF

best subset selection è un estimator!

Due to com- putational cost, the most used criteria to fit a LASSO adjustment are cross-validation tech- niques. Nevertheless, it can be showed that this criterion achieves an adequate λ value for prediction risk, but this leads to inconsistent model selection for sparse methods (see Meinshausen & Bühlmann, 2006)
Int Statistical Rev

Resampling LASSO procedures